# The Math Behind the Magic: Neural Networks, Theory and Practice
## Encuentro Nacional de Ingeniería Matemática 2024
### **Joaquín Fontbona, Javier Maass, Claudio Muñoz y Diego Olguín.**

* Sesión 1: Red neuronal feedforward.[ [Notebook](https://colab.research.google.com/drive/111KBDu5xadyCN5pge4GbJYVnG5faweR_?usp=sharing) ] [ [Slides](https://colab.research.google.com/drive/111KBDu5xadyCN5pge4GbJYVnG5faweR_?usp=sharing) ]**
    - Problema de aprendizaje y solución para problema de regresión.
    - Definición de una red neuronal feedforward.
    - Teorema de aproximación universal.
    - Descenso de gradiente estocástico.
    - Tutoriales en PyTorch: creación de redes neuronales feedforward, visualización de aproximación y de optimización vía redes suficientemente anchas con descenso de gradiente estocástico. Aplicación a clasificación de imágenes y motivación de redes neuronales convolucionales.
 
* Sesión 2: Redes neuronales convolucionales.
    - Aplicación de PINNs.
    - Redes convoluciones (CNN): las imágenes con otros ojos.
    - Aplicación de CNN.
 
* Sesión 3: Todo es una red neuronal feedforward.
    - Physics Informed Neural Networks (PINNs): más allá de la información de los datos.
    - DeepONets: una red para dominar muchas ecuaciones.
    - Generative Adversarial Networks: el comienzo de la inteligencia artificial generativa.
